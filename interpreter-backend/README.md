# Interpreter Backend

Welcome to the Node.js/TypeScript backend for the Interpreter app! This backend manages user authentication, WebSocket connections for real-time communication, conversation state (linking clinicians and patients), processes streamed audio via OpenAI for transcription, and orchestrates translation and action detection.

## Getting Started

Here's how to get things set up.

### What You Need

1.  **Node.js:** Make sure you have Node.js installed (v18+ is good).
2.  **Google Cloud SDK (`gcloud`):** You'll need this for the Cloud SQL part if deploying. Install it from [here](https://cloud.google.com/sdk/docs/install).
3.  **FFmpeg:** The backend requires FFmpeg to be installed on the system for audio format conversion. The `@ffmpeg-installer/ffmpeg` package helps locate it, but FFmpeg itself must be present. You can typically install it via your system's package manager (e.g., `brew install ffmpeg` on macOS, `sudo apt update && sudo apt install ffmpeg` on Debian/Ubuntu).

### Setup Steps

**Permissions Note:** Sometimes shell scripts lose their execute permission. If you get errors running `.sh` files later, just run `chmod +x *.sh` in this directory first.

1.  **Log in to Google Cloud (If using Cloud SQL):**
    ```bash
    gcloud auth login
    gcloud auth application-default login
    ```
2.  **Set Your GCP Project (If using Cloud SQL):**
    ```bash
    gcloud config set project YOUR_PROJECT_ID
    ```
3.  **Install Node Packages:**

    ```bash
    npm install
    ```

    This installs all Node.js dependencies, including `fluent-ffmpeg`, `@ffmpeg-installer/ffmpeg`, `ws`, and `prisma`.

4.  **Configure Environment Variables:**
    Create a `.env` file in the `interpreter-backend` directory (or copy `.env.local` if generated by `db:setup`). Ensure it contains the following essential variables:

    ```dotenv
    # Database URL (Defaults to local SQLite if not using Cloud SQL setup)
    DATABASE_URL="file:./prisma/dev.db"

    # Port for the backend server
    PORT=8080

    # Secret key for signing JWT tokens (Generate using ./generate-jwt-config.sh)
    JWT_SECRET=your_generated_jwt_secret

    # OpenAI API Key (Required for transcription)
    OPENAI_API_KEY=your_openai_api_key
    ```

5.  **Setup Cloud DB (Optional - Defaults to Local SQLite):**
    This script handles Google Cloud SQL setup. It's optional if you plan to only use local SQLite.

    - It checks for (or creates) a Cloud SQL PostgreSQL instance.
    - **Heads up:** It allows connections from _any_ IP (0.0.0.0/0) for easier testing. **Don't use this in production!** It's insecure.
    - It generates credentials for Cloud SQL.
    - It puts the Cloud SQL URL in `.env`.
    - It creates `.env.local` for your local setup, defaulting to SQLite.
    - It makes sure `.env.local` is ignored by Git.

    Run the script (if needed):

    ```bash
    npm run db:setup [your-instance-name] # Creates/updates .env and .env.local
    ```

    **Note:** Ensure your `OPENAI_API_KEY` is added to the `.env` file after running this if it wasn't present.

6.  **Configure Local DB (SQLite - Default):**

    - The project defaults to using SQLite locally via the `.env` file (`DATABASE_URL="file:./prisma/dev.db"`).
    - Ensure the `provider` in `prisma/schema.prisma` under `datasource db` is set to `"sqlite"` for local development.

7.  **Run Local Migration:**
    This creates your local SQLite database file (`prisma/dev.db`) and sets up the tables based on the schema. The core models include `User`, `Patient`, `Conversation`, `Message`, and `Action`.

    ```bash
    npm run prisma:migrate
    ```

8.  **Generate JWT Config (if not done via db:setup):**
    This creates a secret key for signing tokens and saves it in `.env`. Run this if you didn't use `npm run db:setup` or if `JWT_SECRET` is missing.
    ```bash
    ./generate-jwt-config.sh
    # Or add it as an npm script if you prefer: npm run jwt:generate
    ```
    **Remember:** For production, use Secret Manager for your JWT secret, not the `.env` file!

### Run Locally

Now you can start the development server:

```bash
npm run dev
```

It should be running at `http://localhost:8080` (or the port specified in `.env`), using your local SQLite database, and ready to accept WebSocket connections.

## Core Functionality Overview

- **Authentication:** Handles user registration and login via JWT.
- **Session Management:** Allows starting new sessions, associating a clinician (`User`) with a `Patient` within a `Conversation`.
- **Real-time Audio Processing:** Accepts audio chunks (typically WebM/Opus from the browser) via a dedicated WebSocket connection per conversation (`/transcription`). It uses a shared, persistent **FFmpeg** process to convert the incoming audio stream into PCM 16-bit mono audio at 24kHz. This PCM audio is then streamed over a single, shared WebSocket connection to **OpenAI's Realtime API** for transcription.
- **Data Persistence:** Saves transcriptions as `Message` records linked to the conversation.
- **(Planned):** Translation of messages, detection of predefined `Action`s from clinician messages, generation of conversation summaries.

## OpenAI Realtime API Integration (via Backend Proxy)

This backend acts as a proxy and processor for OpenAI's Realtime API for speech-to-text:

- **Frontend Connection:** Clients connect to the backend via WebSocket (`/transcription?conversationId=...`).
- **Backend Audio Input:** Receives audio chunks (e.g., WebM/Opus) from the client's MediaRecorder.
- **Audio Conversion:** A persistent **FFmpeg** process converts the incoming audio streams into **PCM 16-bit, 24kHz, mono** format. This is the format required by OpenAI.
- **Backend -> OpenAI Connection:** Maintains a **single, shared** WebSocket connection to `wss://api.openai.com/v1/realtime` authenticated with the `OPENAI_API_KEY`.
- **Audio Forwarding:** Streams the converted PCM audio chunks to the shared OpenAI WebSocket.
- **Session Configuration:** Configures the OpenAI session upon connection using `transcription_session.update`, specifying:
    - Transcription model (`input_audio_transcription.model: "whisper-1"`).
    - Voice activity detection (`turn_detection`).
    - _Note: `input_audio_format` is implicitly handled by sending correct PCM data._
- **Result Handling:** Receives transcription results (deltas, completed segments) from OpenAI and broadcasts them back to the appropriate client WebSocket based on `conversationId`.

This architecture centralizes the OpenAI API key, manages the required audio format conversion, and handles the persistent connection to OpenAI.

## Deployment

This backend is meant to be deployed to Google Cloud Run.

1.  **Set Prisma Provider:** **Crucially**, before building your Docker image for production, change the provider in `prisma/schema.prisma` back to `"postgresql"`.
2.  **Build & Push:** Build your Docker image and push it to a registry (like Google Artifact Registry).
3.  **Secrets:** Put your production database details, JWT secret, and API keys (e.g., `OPENAI_API_KEY`) into Google Secret Manager.
4.  **Deploy:** Use `gcloud run deploy`. Point it to your image, set `NODE_ENV=production`, and inject your secrets as environment variables.

Check `IMPLEMENTATION_PLAN.md` for more deployment details.

## Acknowledgements

Some setup scripts (like `setup-databases.sh` and `generate-jwt-config.sh`) were adapted from or inspired by the [ktor-scaffold](https://github.com/bperin/ktor-scaffold/blob/main/setup-cloud-db.sh) project.
